{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiwihead15/workshop_gdg/blob/main/Write_SQL_with_natural_language_using_VerteX_AI_and_BigQuery_%5Bsolution%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxcU20E6zfMy"
      },
      "source": [
        "## Who am I?\n",
        "\n",
        "Israel Herraiz\n",
        "\n",
        "Strategic Cloud Engineer, Google\n",
        "\n",
        "## Follow me\n",
        "\n",
        "* Twitter: [@herraiz](http://twitter.com/herraiz)\n",
        "* LinkedIn (mention that you were an attendee in this workshop): https://en.linkedin.com/in/herraiz\n",
        "* https://medium.com/@iht"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_FIuEcjrB85K",
        "outputId": "7f145a97-1f7d-4900-c06a-29a2e4c27085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.8/437.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install langchain==0.0.340 --quiet\n",
        "! pip install chromadb==0.4.13 --quiet\n",
        "! pip install google-cloud-bigquery[pandas] --quiet\n",
        "! pip install google-cloud-aiplatform --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIKfW9zEUtLZ"
      },
      "source": [
        "## Vertex configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YqvnALApSYA-"
      },
      "outputs": [],
      "source": [
        "VERTEX_PROJECT = \"SOME_PROJECT_ID\" # @param{type: \"string\"}\n",
        "VERTEX_REGION = \"us-central1\" # @param{type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3AiF1f2Uxu6"
      },
      "source": [
        "## BigQuery configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hk57x-JuSbCP"
      },
      "outputs": [],
      "source": [
        "BIGQUERY_DATASET = \"noaa_tsunami\" # @param{type: \"string\"}\n",
        "BIGQUERY_PROJECT = \"bigquery-public-data\" # @param{type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqUs77DoU0Ho"
      },
      "source": [
        "## Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXnPJNPnVRbK"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=VERTEX_PROJECT, location=VERTEX_REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRZNjRJ4buAp"
      },
      "source": [
        "## Schemas as context for the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKt25sJNV0zW"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import json\n",
        "\n",
        "bq_client = bigquery.Client(project=VERTEX_PROJECT)\n",
        "bq_tables = bq_client.list_tables(dataset=f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}\")\n",
        "schemas = []\n",
        "for bq_table in bq_tables:\n",
        "   t = bq_client.get_table(f\"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{bq_table.table_id}\")\n",
        "   schema_fields = [f.to_api_repr() for f in t.schema]\n",
        "   schema = f\"The schema for table {bq_table.table_id} is the following: \\n```{json.dumps(schema_fields, indent=1)}```\"\n",
        "   schemas.append(schema)\n",
        "\n",
        "print(f\"Found {len(schemas)} tables in dataset {BIGQUERY_PROJECT}:{BIGQUERY_DATASET}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpmSCQ7QZSAY"
      },
      "source": [
        "## Vector store\n",
        "\n",
        "We add the schemas as documents to a vector store, to be added to the prompt later.\n",
        "\n",
        "We will retrieve only one document from the store for the prompt: the most relevant doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd-NuT9McjDd"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = VertexAIEmbeddings()\n",
        "try: # Avoid duplicated documents\n",
        "  vector_store.delete_collection()\n",
        "except:\n",
        "  print(\"No need to clean the vector store\")\n",
        "vector_store = Chroma.from_texts(schemas, embedding=embeddings)\n",
        "n_docs = len(vector_store.get()['ids'])\n",
        "retriever = vector_store.as_retriever(search_kwargs={'k': 1})\n",
        "print(f\"The vector store has {n_docs} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY8OpGPMyeWZ"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD08GTeYyftN"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "query_model = ChatVertexAI(model_name=\"codechat-bison\", max_output_tokens=2048)\n",
        "interpret_data_model = ChatVertexAI(max_output_tokens=2048)\n",
        "agent_model = ChatVertexAI(max_output_tokens=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTJX0iLTwmcM"
      },
      "source": [
        "## Get a SQL query chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti1xFJdMwyh5"
      },
      "outputs": [],
      "source": [
        "SQL_PROMPT = \"\"\"You are a SQL and BigQuery expert.\n",
        "\n",
        "Your job is to create a query for BigQuery in SQL.\n",
        "\n",
        "The following paragraph contains the schema of the table used for a query. It is encoded in JSON format.\n",
        "\n",
        "{context}\n",
        "\n",
        "Create a BigQuery SQL query for the following user input, using the above table.\n",
        "\n",
        "The user and the agent have done this conversation so far:\n",
        "{chat_history}\n",
        "\n",
        "Follow these restrictions strictly:\n",
        "- Only return the SQL code.\n",
        "- Do not add backticks or any markup. Only write the query as output. NOTHING ELSE.\n",
        "- In FROM, always use the full table path, using `{project}` as project and `{dataset}` as dataset.\n",
        "- Always transform country names to full uppercase. For instance, if the country is Japan, you should use JAPAN in the query.\n",
        "\n",
        "User input: {question}\n",
        "\n",
        "SQL query:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHMBqcbzOa51"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.vectorstore import VectorStoreRetriever\n",
        "def get_documents(retriever: VectorStoreRetriever, question: str) -> str:\n",
        "  # Return only the first document\n",
        "  output = \"\"\n",
        "  for d in retriever.get_relevant_documents(question):\n",
        "    output += d.page_content\n",
        "    output += \"\\n\"\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOBYThDvGiTb"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "\n",
        "### EXERCISE STARTS HERE\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"chat_history\", \"question\", \"project\", \"dataset\"],\n",
        "    template=SQL_PROMPT)\n",
        "\n",
        "partial_prompt = prompt_template.partial(project=BIGQUERY_PROJECT,\n",
        "                                         dataset=BIGQUERY_DATASET)\n",
        "\n",
        "# Input will be like {\"input\": \"SOME_QUESTION\", \"chat_history\": \"HISTORY\"}\n",
        "docs = {\"context\": lambda x: get_documents(retriever, x['input'])}\n",
        "question = {\"question\": itemgetter(\"input\")}\n",
        "chat_history = {\"chat_history\": itemgetter(\"chat_history\")}\n",
        "query_chain = docs | question | chat_history | partial_prompt | query_model\n",
        "query = query_chain | StrOutputParser()\n",
        "### EXERCISE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u4neRJw4HwgA"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "# Example\n",
        "x = {\"input\": \"Which countries in Asia had more houses damaged? Give me the top 3\", \"chat_history\": \"\"}\n",
        "print(query.invoke(x, config={'callbacks': [ConsoleCallbackHandler()]}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Zha4ZCQFSU"
      },
      "source": [
        "## Add more outputs to the previous chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEdgWhHnQJBe"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "def _dict_to_json(x: dict) -> str:\n",
        "  return \"```\\n\" + json.dumps(x) + \"\\n```\"\n",
        "\n",
        "query_response_schema = [\n",
        "    ResponseSchema(name=\"query\", description=\"SQL query to solve the user question.\"),\n",
        "    ResponseSchema(name=\"question\", description=\"Question asked by the user.\"),\n",
        "    ResponseSchema(name=\"context\", description=\"Documents retrieved from the vector store.\")\n",
        "  ]\n",
        "query_output_parser = StructuredOutputParser.from_response_schemas(query_response_schema)\n",
        "query_output_json = docs | question | {\"query\": query} | RunnableLambda(_dict_to_json) | StrOutputParser()\n",
        "query_output = query_output_json | query_output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MCQTMjDvQXeu"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "x = {\"input\": \"Which countries in Asia had more houses damaged? Give me the top 3\", \"chat_history\": \"\"}\n",
        "query_output.invoke(x)  # Output is now a dictionary, input for the next chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7mvJMXqOML9"
      },
      "source": [
        "# Interpret the output chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA4nJ2OTP8ej"
      },
      "outputs": [],
      "source": [
        "INTERPRET_PROMPT = \"\"\"You are a BigQuery expert. You are also expert in extracting data from CSV.\n",
        "\n",
        "The following paragraph describes the schema of the table used for a query. It is encoded in JSON format.\n",
        "\n",
        "{context}\n",
        "\n",
        "A user asked this question:\n",
        "{question}\n",
        "\n",
        "To find the answer, the following SQL query was run in BigQuery:\n",
        "```\n",
        "{query}\n",
        "```\n",
        "\n",
        "The result of that query was the following table in CSV format:\n",
        "```\n",
        "{result}\n",
        "```\n",
        "\n",
        "Based on those results, provide a brief answer to the user question.\n",
        "\n",
        "Follow these restrictions strictly:\n",
        "- Do not add any explanation about how the answer is obtained, just write the answer.\n",
        "- Extract any value related to the answer only from the result of the query. Do not use any other data source.\n",
        "- Just write the answer, omit the question from your answer, this is a chat, just provide the answer.\n",
        "- If you cannot find the answer in the result, do not make up any data, just say \"I cannot find the answer\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFeZ8zvwQAPD"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "def get_bq_csv(bq_client: bigquery.Client, query: str) -> str:\n",
        "  df = bq_client.query(query, location=\"US\").to_dataframe()\n",
        "  return df.to_csv(index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtbGdTCDUlLz"
      },
      "outputs": [],
      "source": [
        "# Get the output of the previous chain\n",
        "\n",
        "### EXERCISE STARTS HERE\n",
        "query = {\"query\": itemgetter(\"query\")}\n",
        "context = {\"context\": itemgetter(\"context\")}\n",
        "question = {\"question\": itemgetter(\"question\")}\n",
        "query_result = {\"result\": lambda x: get_bq_csv(bq_client, x[\"query\"])}\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"query\", \"result\", \"context\"],\n",
        "    template=INTERPRET_PROMPT)\n",
        "\n",
        "run_bq_chain = context | question | query | query_result | prompt\n",
        "run_bq_result = run_bq_chain | interpret_data_model | StrOutputParser()\n",
        "### EXERCISE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjzena1YVFwN"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "x = {\"input\": \"Which countries in Asia had more houses damaged? Give me the top 3\", \"chat_history\": \"\"}\n",
        "print(run_bq_result.invoke(query_output.invoke(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj3XlntDVzV2"
      },
      "source": [
        "# Agent: putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_89yVG3axiN"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "agent_memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    k=10,\n",
        "    return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31F8tq_tXLsc"
      },
      "outputs": [],
      "source": [
        "AGENT_PROMPT = \"\"\"You are a very powerful assistant that can answer questions using BigQuery.\n",
        "\n",
        "You can invoke the tool user_question_tool to answer questions using BigQuery.\n",
        "\n",
        "You can invoke the tool Calculator if you need to do mathematical operations.\n",
        "\n",
        "Always use the tools to try to answer the questions. Use the chat history for context. Never try to use any other external information.\n",
        "\n",
        "Assume that the user may write with misspellings, fix the spelling of the user before passing the question to any tool.\n",
        "\n",
        "Don't mention what tool you have used in your answer.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8X1LGzzXUcw"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMMathChain\n",
        "from langchain.tools import Tool\n",
        "\n",
        "math_chain = LLMMathChain.from_llm(llm=agent_model)\n",
        "math_tool = Tool(\n",
        "  name=\"Calculator\",\n",
        "  description=\"Useful for when you need to answer questions about math.\",\n",
        "  func=math_chain.run,\n",
        "  coroutine=math_chain.arun)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ8UAYRKYC_j"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "@tool\n",
        "def user_question_tool(question) -> str:\n",
        "  \"\"\"Useful to answer natural language questions from users using BigQuery.\"\"\"\n",
        "  config={'callbacks': [ConsoleCallbackHandler()]}\n",
        "  config = {}\n",
        "  memory = agent_memory.buffer_as_str.strip()\n",
        "  question = {\"input\": question, \"chat_history\": memory}\n",
        "  query = query_output.invoke(question, config=config)\n",
        "  print(\"\\n\\n******************\\n\\n\")\n",
        "  print(query['query'])\n",
        "  print(\"\\n\\n******************\\n\\n\")\n",
        "  result = run_bq_result.invoke(query, config=config)\n",
        "  return result.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkKHQn3baaEi"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
        "\n",
        "agent_kwgards = {\"system_message\": AGENT_PROMPT}\n",
        "agent_tools = [math_tool, user_question_tool]\n",
        "\n",
        "agent_memory.clear()\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=agent_tools,\n",
        "    llm=agent_model,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    memory=agent_memory,\n",
        "    agent_kwgards=agent_kwgards,\n",
        "    max_iterations=5,\n",
        "    early_stopping_method='generate',\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0aM6V9BY6dv"
      },
      "outputs": [],
      "source": [
        "q = \"Which countries had more houses damaged? Give me the top 3\"\n",
        "agent.invoke(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5oWeXVzZG1z"
      },
      "outputs": [],
      "source": [
        "agent_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlSyWZzTbnbG"
      },
      "outputs": [],
      "source": [
        "q = \"Of those countries, which one had more deaths?\"\n",
        "agent.invoke(q)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}